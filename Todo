Manipulation - Code Parsing html
Manipulation - Code lemmatisation français
Manipulation - Code tokentisation français
Manipulation - Code stemmatisation français
Référence - Benoit Sagot webpage about LEFFF
Référence - Topic Classification
Référence - NLP with Tensorflow
Référence - Udacity Nanodegree
Référence - GitHub ClaudeCoulombe FrenchLefffLemmatizer
Référence - Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond
Référence - Google Neural Machine Translation System
Référence - LSTM for machine reading (papier à trouver)
Référence _ Attention is all you need ( papier à trouver)
Référence - LibriSpeech ASR corpus ( dataset )
Référence - THEORY AND PRACTICE OF SPEECH RECOGNITION SYSTEMS ( Carnegie Mellon Univertsity )
Référence - Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling (https://arxiv.org/abs/1412.3555)
Référence - An Empirical Exploration of Recurr ne Translation Architectures ( https://arxiv.org/abs/1703.03906v2 )
Référence - Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition (https://arxiv.org/abs/1610.09975)
Référence - Speech Recognition with Deep Recurrent Neural Networks (https://arxiv.org/abs/1303.5778)
Référence - Sequence to Sequence Learning with Neural Networks (https://arxiv.org/abs/1409.3215)
Référence - Show and Tell: A Neural Image Caption Generator ( https://arxiv.org/abs/1411.4555)
Référence - DRAW: A Recurrent Neural Network For Image Generation ( https://arxiv.org/abs/1502.04623)
Référence - A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering (http://www.aclweb.org/anthology/P15-2116)
Référence - SEQUENCE-TO-SEQUENCE RNNS FOR TEXT SUMMARIZATION (https://pdfs.semanticscholar.org/3fbc/45152f20403266b02c4c2adab26fb367522d.pdf)
Référence - Practical recommendations for gradient-based training of deep architectures ( https://arxiv.org/abs/1206.5533)
Référence - Deep Learning book - selecting hyper parametrs( http://www.deeplearningbook.org/contents/guidelines.html)
Référence - How to choose hyper parameters ( http://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters)
Référence - Efficent Backprop ( http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)
Référence- How to Generate a Good Word Embedding? ( https://arxiv.org/abs/1507.05523)
Référence - Systematic evaluation of CNN advances on the ImageNet ( https://arxiv.org/abs/1606.02228)
Référence - Visualizing and Understanding Recurrent Networks (https://arxiv.org/abs/1506.02078)
Concept - Hidden Markov Model appliquée au speech tagging
Concept - Latent Dirichlet Allocation appliquée à classification de document par domaine
Concept - Reccurent Neural Network ( RNN )
Concept - Long Short Term Memory Network ( LSTM )
Concept - Gated Reccurent Unit ( GRU )
Concept - TFDF
Concept - Bag of Words
Concept - Sequence to Sequence ( RNN )
Concept - Embedded RNN
Concept - bi directionel RNN
Concept - encoder - decoder RNN
Concept - Connectionist Temporal Classification  ( CTC )
Concept - N-grams
Cas d'usage - Speech Tagging
Cas d'usage - Classification de document par domaine
Cas d'usage - Analyze de sentiment
Cas d'usage - Machine Translation
Cas d'usage - Speech recognition
Librairies - Gensim
Librairies - NLTK
Librairies - SpaCy
Librairies - Keras
Librairies - Tensorflow
Services - IBM Watson
Services - Google Dialogflow
Projets - Parser wikipedia et se construire un dictionaire en ligne
Projets - Construire un multi model NLP transformant du texte en une modélisation oriénté objet
              - architecture à base de parseur html, module de conversion de ts les concepts, coeur LSTM customisé
Contact - Benoit Sagot, linguiste français
Contact - Jay  Alammar,interactive explorations of neural networks ( http://jalammar.github.io/)
Contact - Illya Suskever ( OpenAI, GoogleBrain) - attention
Divers - GPU
Divers - les concepts du livre NLP with Tensorflow
Divers - Top3 NLP librairies ( https://www.kaggle.com/mjbahmani/top-3-nlp-libraries-tutorial-nltk-spacy-gensim)
